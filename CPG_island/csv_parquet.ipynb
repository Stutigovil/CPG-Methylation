{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "197a69d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1681375674.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    **Dimensions:** ~480k Columns (CpG Beta Values).\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "# CSV to Parquet Conversion (Iterative)\n",
    "**Dataset:** ~1890 samples, split into batches of 10.\n",
    "**Dimensions:** ~480k Columns (CpG Beta Values).\n",
    "**Optimization:** Using `float32` for memory efficiency and `PyArrow` for iterative writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1db1153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: genomics_data.parquet\n",
      "Found 189 CSV files to process.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Use *.csv to grab all files in the folder\n",
    "input_path = \"C:/Users/Stuti/Desktop/Projects/CGP/sample_batches/*.csv\" \n",
    "\n",
    "# Output filename\n",
    "output_file = \"genomics_data.parquet\"\n",
    "\n",
    "# Get list of files\n",
    "csv_files = sorted(glob.glob(input_path))\n",
    "\n",
    "print(f\"Target: {output_file}\")\n",
    "print(f\"Found {len(csv_files)} CSV files to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abae28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversion...\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "parquet_writer = None\n",
    "schema = None\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Starting conversion...\")\n",
    "\n",
    "for i, file in enumerate(csv_files):\n",
    "    try:\n",
    "        # 1. Read CSV (Engine 'c' is faster)\n",
    "        df = pd.read_csv(file, engine='c')\n",
    "        \n",
    "        # 2. Optimization: Downcast floats\n",
    "        # CpG Beta values (0-1) do not need 64-bit precision. \n",
    "        # Float32 cuts RAM and Disk usage by 50%.\n",
    "        fcols = df.select_dtypes('float').columns\n",
    "        df[fcols] = df[fcols].astype('float32')\n",
    "        \n",
    "        # 3. Convert to PyArrow Table\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        \n",
    "        # 4. Initialize Writer (Only on the first file)\n",
    "        if parquet_writer is None:\n",
    "            schema = table.schema\n",
    "            # ZSTD provides excellent compression for genomics data\n",
    "            parquet_writer = pq.ParquetWriter(output_file, schema, compression='zstd')\n",
    "        \n",
    "        # 5. Schema Check (Safety net)\n",
    "        # Ensures that if column order changes in later CSVs, they are aligned\n",
    "        if not table.schema.equals(schema):\n",
    "            table = table.cast(schema)\n",
    "\n",
    "        # 6. Write to Parquet\n",
    "        parquet_writer.write_table(table)\n",
    "        \n",
    "        # Optional: Print progress every 10 files to keep log clean\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i+1}/{len(csv_files)} files...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "        # Decide if you want to break or continue. \n",
    "        # usually better to stop and fix data issues.\n",
    "        break\n",
    "\n",
    "# 7. Close Writer\n",
    "if parquet_writer:\n",
    "    parquet_writer.close()\n",
    "\n",
    "end_time = time.time()\n",
    "duration = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"--- DONE ---\")\n",
    "print(f\"Total time: {duration:.2f} minutes\")\n",
    "print(f\"File saved as: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d7810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the result\n",
    "import os\n",
    "\n",
    "file_size = os.path.getsize(output_file) / (1024 * 1024 * 1024) # Size in GB\n",
    "print(f\"Final File Size: {file_size:.2f} GB\")\n",
    "\n",
    "# Read metadata only (fast)\n",
    "parquet_file = pq.ParquetFile(output_file)\n",
    "print(f\"Total Rows: {parquet_file.metadata.num_rows}\")\n",
    "print(f\"Total Columns: {parquet_file.metadata.num_columns}\")\n",
    "print(f\"Row Groups (Batches): {parquet_file.num_row_groups}\")\n",
    "\n",
    "# Optional: Peek at the first 5 rows and 5 columns\n",
    "# We use columns=[list] to avoid loading all 480k columns into RAM\n",
    "first_few_cols = parquet_file.schema.names[:5] \n",
    "subset = pd.read_parquet(output_file, columns=first_few_cols).head()\n",
    "print(\"\\nFirst 5 rows (subset):\")\n",
    "print(subset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
